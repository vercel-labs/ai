---
title: OpenAI Compatible Providers
description: Use OpenAI compatible providers with the AI SDK.
---

# OpenAI Compatible Providers

You can use the [OpenAI Compatible Provider](https://www.npmjs.com/package/@ai-sdk/openai-compatible) package to use language model providers that implement the OpenAI API.

Below we focus on the general setup and provider instance creation. You can also [write a custom provider package leveraging the OpenAI Compatible package](/providers/openai-compatible-providers/custom-providers).

We provide detailed documentation for the following OpenAI compatible providers:

- [Perplexity](/providers/openai-compatible-providers/perplexity)
- [LM Studio](/providers/openai-compatible-providers/lmstudio)
- [Baseten](/providers/openai-compatible-providers/baseten)

The general setup and provider instance creation is the same for all of these providers.

## Setup

The OpenAI Compatible provider is available via the `@ai-sdk/openai-compatible` module. You can install it with:

<Tabs items={['pnpm', 'npm', 'yarn']}>
  <Tab>
    <Snippet text="pnpm add @ai-sdk/openai-compatible" dark />
  </Tab>
  <Tab>
    <Snippet text="npm install @ai-sdk/openai-compatible" dark />
  </Tab>
  <Tab>
    <Snippet text="yarn add @ai-sdk/openai-compatible" dark />
  </Tab>
</Tabs>

## Provider Instance

To use an OpenAI compatible provider, you can create a custom provider instance with the `createOpenAICompatible` function from `@ai-sdk/openai-compatible`:

```ts
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

const provider = createOpenAICompatible({
  name: 'provider-name',
  apiKey: process.env.PROVIDER_API_KEY,
  baseURL: 'https://api.provider.com/v1',
});
```

You can use the following optional settings to customize the provider instance:

- **baseURL** _string_

  Set the URL prefix for API calls.

- **apiKey** _string_

  API key for authenticating requests. If specified, adds an `Authorization`
  header to request headers with the value `Bearer <apiKey>`. This will be added
  before any headers potentially specified in the `headers` option.

- **headers** _Record&lt;string,string&gt;_

  Optional custom headers to include in requests. These will be added to request headers
  after any headers potentially added by use of the `apiKey` option.

- **queryParams** _Record&lt;string,string&gt;_

  Optional custom url query parameters to include in request urls.

- **fetch** _(input: RequestInfo, init?: RequestInit) => Promise&lt;Response&gt;_

  Custom [fetch](https://developer.mozilla.org/en-US/docs/Web/API/fetch) implementation.
  Defaults to the global `fetch` function.
  You can use it as a middleware to intercept requests,
  or to provide a custom fetch implementation for e.g. testing.

## Language Models

You can create provider models using a provider instance.
The first argument is the model id, e.g. `model-id`.

```ts
const model = provider('model-id');
```

### Example

You can use provider language models to generate text with the `generateText` function:

```ts
import { createOpenAICompatible } from '@ai-sdk/openai-compatible'
import { generateText } from 'ai'

const provider = createOpenAICompatible({
  name: 'provider-name',
  apiKey: process.env.PROVIDER_API_KEY,
  baseURL: 'https://api.provider.com/v1'
})

const { text } = await generateText({
  model: provider('model-id')
  prompt: 'Write a vegetarian lasagna recipe for 4 people.'
})
```

### Including model ids for auto-completion

```ts
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';
import { generateText } from 'ai';

type ExampleChatModelIds =
  | 'meta-llama/Llama-3-70b-chat-hf'
  | 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'
  | (string & {});

type ExampleCompletionModelIds =
  | 'codellama/CodeLlama-34b-Instruct-hf'
  | 'Qwen/Qwen2.5-Coder-32B-Instruct'
  | (string & {});

type ExampleEmbeddingModelIds =
  | 'BAAI/bge-large-en-v1.5'
  | 'bert-base-uncased'
  | (string & {});

const model = createOpenAICompatible<
  ExampleChatModelIds,
  ExampleCompletionModelIds,
  ExampleEmbeddingModelIds
>({
  name: 'example',
  apiKey: process.env.PROVIDER_API_KEY,
  baseURL: 'https://api.example.com/v1',
});

// Subsequent calls to e.g. `model.chatModel` will auto-complete the model id
// from the list of `ExampleChatModelIds` while still allowing free-form
// strings as well.

const { text } = await generateText({
  model: model.chatModel('meta-llama/Llama-3-70b-chat-hf'),
  prompt: 'Write a vegetarian lasagna recipe for 4 people.',
});
```

### Custom query parameters

Some providers may require custom query parameters. An example is the [Azure AI
Model Inference
API](https://learn.microsoft.com/en-us/azure/machine-learning/reference-model-inference-chat-completions?view=azureml-api-2)
which requires an `api-version` query parameter.

You can set these via the optional `queryParams` provider setting. These will be
added to all requests made by the provider.

```ts highlight="7-9"
import { createOpenAICompatible } from '@ai-sdk/openai-compatible';

const provider = createOpenAICompatible({
  name: 'provider-name',
  apiKey: process.env.PROVIDER_API_KEY,
  baseURL: 'https://api.provider.com/v1',
  queryParams: {
    'api-version': '1.0.0',
  },
});
```

For example, with the above configuration, API requests would include the query parameter in the URL like:
`https://api.provider.com/v1/chat/completions?api-version=1.0.0`.

## Metadata Processors

The OpenAI Compatible provider supports extracting and processing provider-specific metadata from API responses through metadata processors. These processors allow you to capture additional information returned by the provider beyond the standard response format.

Metadata processors receive the raw, unprocessed response data from the provider, giving you complete flexibility to extract any custom fields or experimental features that the provider may include. This is particularly useful when:

- Working with providers that include non-standard response fields
- Experimenting with beta or preview features
- Capturing provider-specific metrics or debugging information
- Supporting rapid provider API evolution without SDK changes

Metadata processors work with both streaming and non-streaming chat completions and consist of two main components:

1. A function to extract metadata from complete responses
2. A streaming processor that can accumulate metadata across chunks in a streaming response

Here's an example metadata processor that captures both standard and custom provider data:

```typescript
const MyMetadataProcessor: MetadataProcessor = {
  // Process complete, non-streaming responses
  buildMetadataFromResponse: (response: unknown) => {
    // You have access to the complete raw response
    // Extract any fields the provider includes
    return {
      myProvider: {
        standardUsage: response.usage,
        experimentalFeatures: response.beta_features,
        customMetrics: {
          processingTime: response.server_timing?.total_ms,
          modelVersion: response.model_version,
          // ... any other provider-specific data
        },
      },
    };
  },

  // Process streaming responses
  createStreamingMetadataProcessor: () => {
    let accumulatedData = {
      timing: [],
      customFields: {},
    };

    return {
      // Process each chunk's raw data
      processChunk: (chunk: unknown) => {
        if (chunk.server_timing) {
          accumulatedData.timing.push(chunk.server_timing);
        }
        if (chunk.custom_data) {
          Object.assign(accumulatedData.customFields, chunk.custom_data);
        }
      },
      // Build final metadata from accumulated data
      buildFinalMetadata: () => ({
        myProvider: {
          streamTiming: accumulatedData.timing,
          customData: accumulatedData.customFields,
        },
      }),
    };
  },
};
```

You can provide a metadata processor when creating your provider instance:

```typescript
const provider = createOpenAICompatible({
  name: 'my-provider',
  apiKey: process.env.PROVIDER_API_KEY,
  baseURL: 'https://api.provider.com/v1',
  metadataProcessor: MyMetadataProcessor,
});
```

The extracted metadata will be included in the response under the `providerMetadata` field:

```typescript
const { text, providerMetadata } = await generateText({
  model: provider('model-id'),
  prompt: 'Hello',
});

console.log(providerMetadata.myProvider.customMetric);
```

This allows you to access provider-specific information while maintaining a consistent interface across different providers.

### Type-Safe Metadata Processing with Zod

When working with provider-specific response data, it's recommended to use a schema validation library like [Zod](https://zod.dev) to ensure type safety and handle potential inconsistencies in the response data. Here's a simplified example:

```typescript
import { MetadataProcessor } from '@ai-sdk/openai-compatible';
import { z } from 'zod';

// Define schemas for expected response shapes
const UsageSchema = z.object({
  cache_hit_tokens: z.number().nullish(),
  cache_miss_tokens: z.number().nullish(),
});

const ResponseSchema = z.object({
  usage: UsageSchema.nullish(),
});

const StreamChunkSchema = z.object({
  choices: z
    .array(
      z.object({
        finish_reason: z.string().nullish(),
      }),
    )
    .optional(),
  usage: UsageSchema.nullish(),
});

// Create a type-safe metadata processor
const TypeSafeMetadataProcessor: MetadataProcessor = {
  buildMetadataFromResponse: (response: unknown) => {
    // Safely parse the raw response
    const parsed = ResponseSchema.safeParse(response);
    if (!parsed.success || !parsed.data.usage) return undefined;

    return {
      provider: {
        cacheHitTokens: parsed.data.usage.cache_hit_tokens ?? NaN,
        cacheMissTokens: parsed.data.usage.cache_miss_tokens ?? NaN,
      },
    };
  },

  createStreamingMetadataProcessor: () => {
    let finalUsage: z.infer<typeof UsageSchema> | undefined;

    return {
      processChunk: (chunk: unknown) => {
        const parsed = StreamChunkSchema.safeParse(chunk);
        if (!parsed.success) return;

        // Capture usage data from the final chunk
        if (parsed.data.choices?.[0]?.finish_reason === 'stop') {
          finalUsage = parsed.data.usage;
        }
      },
      buildFinalMetadata: () => {
        if (!finalUsage) return undefined;
        return {
          provider: {
            cacheHitTokens: finalUsage.cache_hit_tokens ?? NaN,
            cacheMissTokens: finalUsage.cache_miss_tokens ?? NaN,
          },
        };
      },
    };
  },
};
```

Using Zod schemas provides several benefits:

- Type safety and runtime validation of response data
- Clear documentation of expected response structure
- Graceful handling of missing or nullable fields
- TypeScript inference from schema definitions

This approach is particularly valuable when working with experimental or evolving provider APIs, as it helps catch and handle response format changes early in development.
